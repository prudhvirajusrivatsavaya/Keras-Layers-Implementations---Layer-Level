{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7daea78-0602-4518-80b7-c0791e3202f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c23b53-6923-4498-9b74-de2bda591ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbatched input.\n",
    "tokenizer = keras_nlp.models.AlbertTokenizer.from_preset(\"albert_base_en_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ea0801-fa00-4506-bfea-a7ff11a12a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([  13,    1,  438, 2231,  886, 2385, 4298,    9])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1232c8c0-aac4-420e-a979-ab641454cd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[13, 1, 438, 2231, 886, 2385, 4298, 9], [13, 1, 438, 2385, 7132, 9]]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched input.\n",
    "tokenizer([\"The quick brown fox jumped.\", \"The fox slept.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578d14d0-a2c4-4765-9191-6c1b9b634cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b' \\xe2\\x81\\x87 he quick brown fox jumped.'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detokenization.\n",
    "tokenizer.detokenize(tokenizer(\"The quick brown fox jumped.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2afe17ab-5b01-4e2f-93f1-3840df0f423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom vocabulary.\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "#from keras_nlp import sentencepiece\n",
    "bytes_io = io.BytesIO()\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"The quick brown fox jumped.\"])\n",
    "sentencepiece.SentencePieceTrainer.train(\n",
    "    sentence_iterator=ds.as_numpy_iterator(),\n",
    "    model_writer=bytes_io,\n",
    "    vocab_size=10,\n",
    "    model_type=\"WORD\",\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece=\"<pad>\",\n",
    "    unk_piece=\"<unk>\",\n",
    "    bos_piece=\"[CLS]\",\n",
    "    eos_piece=\"[SEP]\",\n",
    "    user_defined_symbols=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4811f615-0e1e-481b-adf9-fd3a0a64dd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([5, 9, 6, 7, 8])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras_nlp.models.AlbertTokenizer(\n",
    "    proto=bytes_io.getvalue(),\n",
    ")\n",
    "tokenizer(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad866597-6e3c-49f8-a161-d4b6cf7f2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.AlbertPreprocessor.from_preset(\n",
    "    \"albert_base_en_uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011dd92b-78fb-4d8a-8c55-dd6b4258f9e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       " array([   2,   13,    1,  438, 2231,  886, 2385, 4298,    9,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0])>,\n",
       " 'segment_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])>,\n",
       " 'padding_mask': <tf.Tensor: shape=(512,), dtype=bool, numpy=\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and pack a single sentence.\n",
    "preprocessor(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06824cb7-37d7-4336-a0b9-31b53a98c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
       " array([[ 2, 13,  1, ...,  0,  0,  0],\n",
       "        [ 2, 13,  1, ...,  0,  0,  0]])>,\n",
       " 'segment_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])>,\n",
       " 'padding_mask': <tf.Tensor: shape=(2, 512), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False]])>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize a batch of single sentences.\n",
    "preprocessor([\"The quick brown fox jumped.\", \"Call me Ishmael.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de94cbb-c0b6-4ab5-b43c-cad4f71356db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16861cf-1c40-4615-bd80-c14f5c1d15af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_output': <tf.Tensor: shape=(1, 12, 768), dtype=float32, numpy=\n",
       " array([[[-0.90249944, -1.7424964 ,  0.48982424, ...,  0.5020634 ,\n",
       "           0.07291413,  2.2348678 ],\n",
       "         [-0.56565154, -1.9457185 ,  0.84350604, ...,  0.18744577,\n",
       "          -0.44964558,  2.3078506 ],\n",
       "         [-0.82057583, -2.1958494 ,  0.47708926, ...,  0.87245214,\n",
       "           0.08581122,  1.6888585 ],\n",
       "         ...,\n",
       "         [-1.142025  , -1.9482069 ,  0.31074753, ...,  0.6979596 ,\n",
       "          -0.05349603,  1.3303281 ],\n",
       "         [-0.55054593, -2.0325682 ,  1.0123701 , ...,  0.46946406,\n",
       "          -0.6558784 ,  1.9517909 ],\n",
       "         [-0.40875772, -1.8813509 ,  0.8480543 , ...,  0.36955604,\n",
       "          -1.0256791 ,  2.107257  ]]], dtype=float32)>,\n",
       " 'pooled_output': <tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       " array([[ 1.63835377e-01, -6.00778401e-01, -2.41029114e-01,\n",
       "          4.42382246e-01,  5.23522198e-01, -6.44949257e-01,\n",
       "          1.76148340e-01,  6.20967329e-01, -3.22868794e-01,\n",
       "         -3.58818442e-01, -7.93024242e-01, -1.63770795e-01,\n",
       "          6.22734785e-01, -5.96319675e-01,  1.13190658e-01,\n",
       "          2.98576444e-01,  3.21784884e-01, -5.76379359e-01,\n",
       "         -1.93247959e-01,  4.65169609e-01,  2.18511581e-01,\n",
       "          5.63912034e-01, -3.39422256e-01,  6.86986089e-01,\n",
       "          3.67558986e-01,  3.02139401e-01, -2.94790059e-01,\n",
       "          4.82012294e-02, -6.87408745e-01,  3.66271645e-01,\n",
       "          1.07545882e-01, -5.59908986e-01,  4.28308368e-01,\n",
       "          9.49317366e-02,  4.53671753e-01, -1.32985801e-01,\n",
       "          4.83886749e-01, -1.40953064e-01, -4.31936592e-01,\n",
       "         -1.38494655e-01, -1.88779920e-01,  2.31730536e-01,\n",
       "         -1.64196953e-01,  5.50748050e-01, -4.23159212e-01,\n",
       "         -2.61961728e-01, -2.25070834e-01, -1.02509670e-01,\n",
       "         -2.07137018e-01,  3.87025535e-01,  6.43444136e-02,\n",
       "          3.05350810e-01,  5.96651852e-01,  2.39230454e-01,\n",
       "         -2.20389627e-02, -2.36713588e-01,  8.07444081e-02,\n",
       "          3.05927217e-01,  1.79761797e-01, -1.64045572e-01,\n",
       "         -1.25588834e-01, -1.79317698e-01, -5.11068523e-01,\n",
       "         -4.47250247e-01,  4.55255359e-01,  7.65601099e-01,\n",
       "         -7.53255635e-02, -9.31484774e-02,  3.01569879e-01,\n",
       "         -1.70323297e-01,  1.37039274e-01, -2.62996294e-02,\n",
       "          3.76084470e-03, -4.89706635e-01, -2.56696761e-01,\n",
       "          6.01088703e-01, -2.96617150e-01,  1.40126273e-01,\n",
       "          7.40165710e-01, -6.73975945e-01,  1.63618356e-01,\n",
       "          3.62982929e-01, -4.78634685e-01, -7.61040002e-02,\n",
       "         -2.91198753e-02, -4.13281292e-01, -4.29550670e-02,\n",
       "         -2.93636054e-01,  5.90029240e-01, -6.68738261e-02,\n",
       "          5.46850502e-01, -1.20258451e-01,  4.51306313e-01,\n",
       "          5.43047071e-01, -3.61461252e-01, -2.00459898e-01,\n",
       "          9.50218737e-02,  2.58064121e-01,  5.17485291e-02,\n",
       "          4.57403898e-01, -6.44693673e-01, -1.94897771e-01,\n",
       "         -3.29371929e-01,  2.89598614e-01,  2.90806025e-01,\n",
       "          6.18189216e-01,  1.53199490e-02, -1.43863499e-01,\n",
       "         -6.59684718e-01,  2.31313393e-01, -5.09449720e-01,\n",
       "          3.23250502e-01,  6.33897632e-02,  2.51001000e-01,\n",
       "         -1.17611818e-01,  3.75476062e-01,  4.86504376e-01,\n",
       "          2.94478714e-01,  2.51739603e-02, -3.40046853e-01,\n",
       "          1.61527060e-02, -1.11903399e-01,  2.54748791e-01,\n",
       "          6.09897614e-01,  9.07192677e-02, -5.35962582e-01,\n",
       "         -2.93698490e-01,  6.47168994e-01,  2.82599241e-01,\n",
       "          1.97729334e-01, -5.98079503e-01,  6.88576162e-01,\n",
       "          6.88742340e-01,  6.05131209e-01, -2.89176255e-01,\n",
       "          3.19264948e-01,  2.29944527e-01, -1.35204926e-01,\n",
       "         -1.31895766e-02, -2.31415741e-02, -1.53841421e-01,\n",
       "         -7.50938773e-01,  7.31137395e-01,  1.37573034e-01,\n",
       "         -2.74659455e-01, -5.41288033e-02, -3.91181298e-02,\n",
       "          1.12504642e-02, -3.84125143e-01,  5.69019258e-01,\n",
       "          1.41943311e-02,  2.41685808e-02, -6.17303312e-01,\n",
       "         -7.41830587e-01,  1.46715179e-01, -2.82191724e-01,\n",
       "          6.45383775e-01,  2.00591147e-01,  2.23319188e-01,\n",
       "         -7.82723486e-01,  4.71979439e-01,  4.66984034e-01,\n",
       "         -6.22429132e-01,  7.00666189e-01, -4.06070501e-01,\n",
       "          4.31079060e-01,  9.18985985e-04,  3.81158024e-01,\n",
       "          5.93784750e-01, -7.07989275e-01, -4.63959575e-01,\n",
       "         -8.18422914e-01, -5.03798604e-01,  1.89904179e-02,\n",
       "         -3.39788020e-01, -9.91471931e-02,  5.52872658e-01,\n",
       "          5.58551431e-01, -3.43238175e-01, -4.65763003e-01,\n",
       "         -3.96159701e-02,  1.92967802e-01, -2.43445426e-01,\n",
       "         -3.25479865e-01, -4.23119575e-01,  2.27030605e-01,\n",
       "         -3.63662332e-01, -4.49073344e-01,  1.09711709e-02,\n",
       "          5.73821604e-01, -6.88630760e-01, -2.72683859e-01,\n",
       "          1.63800389e-01, -4.42633778e-02, -6.62529647e-01,\n",
       "         -3.55304003e-01, -6.02357030e-01, -2.96932459e-02,\n",
       "         -5.08556962e-01,  7.16275811e-01, -3.18040907e-01,\n",
       "          2.55325377e-01,  2.51141369e-01, -7.83821285e-01,\n",
       "          4.00379270e-01,  2.65798151e-01, -3.97436410e-01,\n",
       "         -7.56524861e-01, -4.89493340e-01,  4.90173489e-01,\n",
       "          6.59218788e-01,  5.93466043e-01,  6.48450255e-01,\n",
       "         -8.61363858e-02, -6.63030088e-01, -8.80596973e-03,\n",
       "         -5.57773747e-02,  1.80519506e-01,  4.92585719e-01,\n",
       "          6.13116443e-01,  2.34817471e-02, -2.54981309e-01,\n",
       "         -1.78062022e-01,  1.44044518e-01,  3.89350921e-01,\n",
       "          1.46395072e-01, -1.40297607e-01, -6.91473782e-02,\n",
       "         -6.65423423e-02, -1.80278376e-01,  7.18239486e-01,\n",
       "         -2.61774927e-01, -6.90302670e-01,  2.48391256e-01,\n",
       "          1.04450703e-01, -4.52398583e-02, -4.28259701e-01,\n",
       "          2.33310923e-01, -1.26489922e-01,  4.84494030e-01,\n",
       "          6.33698404e-01,  2.51514971e-01, -1.67494848e-01,\n",
       "          4.16733384e-01, -2.42653877e-01,  5.86673260e-01,\n",
       "         -6.85530186e-01, -1.80150568e-01, -1.69934213e-01,\n",
       "         -1.73526332e-01,  6.90649450e-01,  1.57628208e-01,\n",
       "         -4.13759649e-01, -6.62532151e-01, -1.20523430e-01,\n",
       "          7.08660036e-02,  4.69725668e-01,  2.24849582e-01,\n",
       "          1.23153925e-01, -3.29889596e-01, -6.96531355e-01,\n",
       "          1.82738557e-01,  7.15609133e-01,  5.97242296e-01,\n",
       "          3.71213734e-01,  1.83960959e-01, -1.63544178e-01,\n",
       "         -2.64436662e-01, -7.75526464e-02, -4.64102447e-01,\n",
       "          4.22070026e-01, -7.51559734e-01, -2.30098739e-01,\n",
       "         -2.84078671e-03,  7.84771144e-02,  5.31841040e-01,\n",
       "          1.71457842e-01, -2.31768802e-01,  8.31444934e-02,\n",
       "         -3.19320381e-01, -2.50415504e-01,  4.72109728e-02,\n",
       "          2.24186912e-01, -3.89856279e-01, -7.23720074e-01,\n",
       "          6.57941699e-02, -1.43067241e-01,  3.78673553e-01,\n",
       "          4.07558262e-01,  1.10470220e-01, -2.79784709e-01,\n",
       "         -1.72958076e-01, -4.54682000e-02,  6.14874661e-01,\n",
       "          3.53365600e-01,  6.94873095e-01, -6.60142481e-01,\n",
       "         -4.58320349e-01, -5.84193366e-03, -2.62313455e-01,\n",
       "         -3.94015819e-01, -1.67095363e-01, -6.99708819e-01,\n",
       "          3.43989044e-01, -6.56924129e-01, -1.18583232e-01,\n",
       "         -7.01308906e-01, -6.65575638e-02,  7.99981594e-01,\n",
       "         -5.02088010e-01, -4.80942488e-01,  6.63198352e-01,\n",
       "          3.14087480e-01, -2.60141701e-01, -3.69738460e-01,\n",
       "          4.10639137e-01, -1.80990443e-01, -1.92247152e-01,\n",
       "          4.09697950e-01, -7.89105818e-02, -4.72078323e-02,\n",
       "         -7.48746514e-01,  2.85851687e-01, -6.73526406e-01,\n",
       "         -2.20976710e-01, -5.55659592e-01, -1.95476077e-02,\n",
       "          6.06387377e-01,  6.07848585e-01,  5.46108663e-01,\n",
       "         -6.07872128e-01,  3.60570818e-01, -2.32974932e-01,\n",
       "         -1.82160228e-01, -5.16488433e-01, -3.64967525e-01,\n",
       "         -3.82389545e-01,  2.88893044e-01,  1.03756696e-01,\n",
       "          6.34765178e-02, -3.89237344e-01, -1.32258251e-01,\n",
       "          4.99343872e-01,  6.35184586e-01,  7.57838547e-01,\n",
       "         -2.93108821e-01, -4.31660682e-01,  4.92411643e-01,\n",
       "         -1.98613837e-01, -5.69551349e-01,  2.82757711e-02,\n",
       "          3.75145525e-01, -4.54191238e-01,  5.10267854e-01,\n",
       "         -6.15568459e-01, -3.52870971e-01, -1.93988129e-01,\n",
       "          2.61880308e-02, -8.22426319e-01, -5.82413673e-01,\n",
       "          3.89994532e-01, -3.01031917e-01,  5.01261503e-02,\n",
       "         -1.29927218e-01, -4.74189281e-01, -1.39886975e-01,\n",
       "         -8.60235393e-01, -7.73890197e-01, -4.55674440e-01,\n",
       "          3.70719470e-02,  5.22030354e-01,  7.52298474e-01,\n",
       "          7.73564517e-01,  5.88910997e-01,  7.38421261e-01,\n",
       "          7.50507340e-02,  4.78264540e-01, -9.72842351e-02,\n",
       "          9.42195356e-02,  1.56903371e-01, -2.21021667e-01,\n",
       "         -4.61946994e-01,  1.79660872e-01, -4.43321228e-01,\n",
       "         -6.91840649e-01,  3.37050319e-01, -6.99634612e-01,\n",
       "          1.49277121e-01, -4.52267639e-02,  6.17136836e-01,\n",
       "          6.20683312e-01, -3.30890179e-01,  4.62194264e-01,\n",
       "          1.21495634e-01,  1.71425059e-01,  4.72714573e-01,\n",
       "         -3.17847431e-01, -2.12401301e-01, -6.99849606e-01,\n",
       "         -3.49881709e-01, -1.83970422e-01,  3.48978549e-01,\n",
       "          2.08173543e-01,  1.61137991e-02,  3.03872321e-02,\n",
       "         -7.22193718e-01,  2.63638556e-01, -2.34928012e-01,\n",
       "         -8.51383686e-01, -3.56646240e-01,  1.02805570e-01,\n",
       "         -2.66276866e-01,  4.44508612e-01,  1.94684997e-01,\n",
       "          9.13688764e-02, -3.31820011e-01,  1.48533270e-01,\n",
       "         -5.77945948e-01,  1.29927456e-01, -7.57119954e-02,\n",
       "          8.86481069e-03,  1.44386990e-02, -5.36085784e-01,\n",
       "          1.03221275e-01, -2.02559695e-01,  3.66324753e-01,\n",
       "          4.15809363e-01,  1.87022984e-01,  4.57043797e-01,\n",
       "          4.30245370e-01, -5.47802508e-01,  3.02234590e-01,\n",
       "         -2.83992916e-01,  3.17137390e-01,  4.00294393e-01,\n",
       "         -4.45340663e-01, -1.31640404e-01, -4.44952518e-01,\n",
       "          1.20972767e-01,  5.47259688e-01,  7.57641017e-01,\n",
       "          4.57208335e-01,  1.18919276e-01,  2.32369155e-01,\n",
       "         -4.68909025e-01, -3.92054766e-01,  6.26421869e-01,\n",
       "          4.24574278e-02, -1.10833660e-01,  4.61881548e-01,\n",
       "         -4.96378958e-01, -2.55387604e-01, -1.26476452e-01,\n",
       "          5.55834472e-01, -5.67317605e-01, -2.22390905e-01,\n",
       "          3.17246407e-01,  4.01035041e-01, -2.73064584e-01,\n",
       "         -1.06679194e-01, -2.03160778e-01,  4.26911741e-01,\n",
       "         -5.46488583e-01,  2.63881475e-01,  2.45824724e-01,\n",
       "          5.26832402e-01, -7.20701933e-01,  4.01323974e-01,\n",
       "          2.35532373e-01,  9.26201120e-02,  2.44555950e-01,\n",
       "          4.35190141e-01,  5.60386539e-01,  5.41556835e-01,\n",
       "          6.59179568e-01,  3.25977266e-01,  8.14680159e-01,\n",
       "          3.18387657e-01,  3.03821832e-01, -1.07083611e-01,\n",
       "          4.32632625e-01, -8.32891762e-02, -2.48716906e-01,\n",
       "          2.35191271e-01,  7.00152993e-01, -2.03129888e-01,\n",
       "          3.77225250e-01,  2.67764896e-01,  5.90941682e-02,\n",
       "         -5.55625483e-02,  4.23935018e-02,  1.57717407e-01,\n",
       "         -6.79705143e-02,  5.12102902e-01,  1.19404778e-01,\n",
       "          6.67518973e-01, -1.60935358e-03, -5.16714513e-01,\n",
       "         -3.73982102e-01,  2.99490839e-01,  7.95804858e-01,\n",
       "         -6.48354232e-01, -6.24971509e-01,  2.47220024e-01,\n",
       "          3.17455262e-01,  1.99057937e-01, -5.00612929e-02,\n",
       "         -7.45871425e-01,  8.88089463e-02, -2.90350407e-01,\n",
       "          4.23508495e-01,  5.54197788e-01,  4.29155797e-01,\n",
       "         -2.13116873e-02,  8.05894375e-01, -3.62934291e-01,\n",
       "          4.06111270e-01, -1.61444068e-01, -3.74107838e-01,\n",
       "         -1.56541355e-02, -1.32361934e-01,  7.54130721e-01,\n",
       "         -3.30260508e-02,  5.24094820e-01, -5.30990899e-01,\n",
       "          5.77779114e-01, -1.06699608e-01, -1.70507748e-02,\n",
       "          3.10583021e-02,  6.52877808e-01,  1.18410870e-01,\n",
       "         -1.79106042e-01, -1.66808553e-02, -1.67508259e-01,\n",
       "          1.78176891e-02, -4.73727852e-01, -2.04765037e-01,\n",
       "         -2.01320663e-01,  3.50137293e-01, -2.94744194e-01,\n",
       "          1.12419136e-01,  1.58697098e-01,  2.84780949e-01,\n",
       "          8.99840653e-01, -5.50494850e-01, -7.29425073e-01,\n",
       "          3.07867795e-01,  3.38963047e-02, -6.05550885e-01,\n",
       "          1.31250441e-01, -3.51537526e-01, -9.19090211e-01,\n",
       "         -3.55779767e-01, -4.81795758e-01, -6.58217907e-01,\n",
       "          5.30016422e-01,  2.61029810e-01,  2.12837905e-01,\n",
       "          2.55307287e-01, -1.74562708e-01, -5.33468068e-01,\n",
       "         -3.68931368e-02,  6.21144800e-03, -4.16027494e-02,\n",
       "         -2.11216390e-01,  1.38989896e-01, -4.74439859e-02,\n",
       "         -6.28458977e-01,  3.31073612e-01,  7.67051354e-02,\n",
       "          6.99568152e-01,  5.97305261e-02, -3.41050208e-01,\n",
       "         -2.97442764e-01, -1.74166504e-02,  7.34879434e-01,\n",
       "         -7.04591349e-02,  3.88711601e-01, -1.14500649e-01,\n",
       "         -5.69920182e-01, -1.91322416e-01, -4.88460630e-01,\n",
       "          2.84230858e-01,  2.96781212e-01, -2.39693493e-01,\n",
       "          2.15853110e-01, -4.00049806e-01, -1.47128299e-01,\n",
       "          2.46848434e-01, -2.89285570e-01,  6.84829593e-01,\n",
       "         -6.02197826e-01, -2.41082057e-01, -3.95428836e-01,\n",
       "         -2.53330201e-01, -4.25902277e-01, -7.61640072e-02,\n",
       "          4.75631982e-01, -5.13647079e-01, -6.83047593e-01,\n",
       "         -5.51674485e-01,  3.08722854e-01, -6.02946699e-01,\n",
       "         -3.13806832e-01, -4.84220147e-01, -4.21051711e-01,\n",
       "          6.74028248e-02, -7.12658226e-01,  5.45455575e-01,\n",
       "          9.75575745e-02, -5.07019222e-01, -6.70138955e-01,\n",
       "         -5.67064583e-02,  4.88353550e-01, -4.99303490e-01,\n",
       "         -3.23413134e-01,  6.54386356e-02, -5.33791408e-02,\n",
       "          3.41427356e-01, -2.47642562e-01,  2.84939736e-01,\n",
       "          8.37453127e-01,  7.54643261e-01, -7.68195912e-02,\n",
       "         -1.21261165e-01, -7.95352876e-01,  2.52371162e-01,\n",
       "          3.44867676e-01, -1.99642107e-01, -4.58898067e-01,\n",
       "          1.18299574e-01, -2.23584734e-02, -5.53544223e-01,\n",
       "          3.68366182e-01,  2.14338139e-01,  5.91616273e-01,\n",
       "          2.22136855e-01,  6.68455586e-02, -6.48408383e-02,\n",
       "         -3.79005760e-01, -1.95323620e-02,  2.25136146e-01,\n",
       "         -7.85262465e-01, -4.04730320e-01, -5.71769297e-01,\n",
       "         -8.71941626e-01, -3.43629539e-01, -6.59332797e-02,\n",
       "          2.67754588e-03,  6.85615540e-01,  4.58578795e-01,\n",
       "         -1.68678612e-01,  1.92027494e-01, -4.62083779e-02,\n",
       "          4.41573501e-01, -5.30524194e-01, -5.63605964e-01,\n",
       "         -1.51761249e-01,  1.37224644e-01,  9.02444199e-02,\n",
       "         -2.52721339e-01, -1.15747795e-01,  1.38769254e-01,\n",
       "          3.16741526e-01, -1.73341528e-01, -4.32337224e-01,\n",
       "         -2.80105591e-01,  2.04200923e-01, -5.40059924e-01,\n",
       "          1.75584570e-01,  2.24229172e-01, -7.86098242e-02,\n",
       "         -2.87528783e-02, -1.19097434e-01,  1.21747255e-01,\n",
       "          9.35237855e-02,  1.38523653e-02,  7.61762679e-01,\n",
       "          2.51800090e-01,  6.63088381e-01, -2.09956735e-01,\n",
       "         -3.79154831e-01, -1.34858534e-01,  2.20647559e-01,\n",
       "          3.48591715e-01,  8.56820226e-01, -1.07020721e-01,\n",
       "          4.21622932e-01,  4.37904626e-01,  7.71370173e-01,\n",
       "          5.97730458e-01,  6.38830960e-01, -4.10019189e-01,\n",
       "          1.19724916e-02, -6.13580465e-01,  2.15154126e-01,\n",
       "         -5.22399306e-01, -4.96859998e-01,  4.19870526e-01,\n",
       "         -2.39599571e-02, -5.58716893e-01,  1.96072925e-03,\n",
       "         -1.90060303e-01, -6.13018334e-01,  8.77510309e-02,\n",
       "          5.25106966e-01, -1.64284632e-01, -1.92761779e-01,\n",
       "          6.91075683e-01,  1.24479137e-01, -1.28404215e-01,\n",
       "         -3.06173474e-01, -7.11443961e-01,  4.62188959e-01,\n",
       "         -1.67586669e-01, -1.99304715e-01,  6.17884219e-01,\n",
       "         -4.36211117e-02, -2.92851150e-01,  4.21813905e-01,\n",
       "          5.80710351e-01,  1.88942745e-01, -6.17516518e-01,\n",
       "          3.73393387e-01, -4.30726528e-01,  3.43992501e-01,\n",
       "         -1.87866822e-01,  1.40579551e-01,  3.08050096e-01,\n",
       "          3.23681772e-01,  3.21927816e-01,  1.36266783e-01,\n",
       "          4.29330379e-01,  9.64086317e-03, -2.45825857e-01,\n",
       "         -1.03617817e-01, -5.80972195e-01,  1.56872943e-01,\n",
       "         -1.05619945e-01, -2.33662888e-01,  8.42756808e-01,\n",
       "         -3.27928424e-01, -7.55615950e-01,  4.54660952e-01,\n",
       "         -4.20254081e-01,  2.35988021e-01,  1.39415845e-01,\n",
       "         -4.17304814e-01, -3.19521129e-01,  5.41278183e-01,\n",
       "          2.34988518e-02,  4.80564773e-01, -8.55427384e-01,\n",
       "         -5.82510456e-02,  8.09798002e-01, -2.27340803e-01,\n",
       "          2.98500657e-01,  5.63058615e-01,  2.70987600e-01,\n",
       "          2.72187352e-01, -2.92745322e-01, -7.27136970e-01,\n",
       "         -5.04890382e-01,  4.15794998e-01, -2.02848554e-01,\n",
       "         -2.69362122e-01,  5.27213573e-01,  2.25067347e-01]], dtype=float32)>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "    \"token_ids\": np.ones(shape=(1, 12), dtype=\"int32\"),\n",
    "    \"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]]),\n",
    "    \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),\n",
    "}\n",
    "\n",
    "# Randomly initialized ALBERT encoder\n",
    "model = keras_nlp.models.AlbertBackbone(\n",
    "    vocabulary_size=30000,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    num_groups=1,\n",
    "    num_inner_repetitions=1,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=768,\n",
    "    intermediate_dim=3072,\n",
    "    max_sequence_length=12,\n",
    ")\n",
    "output = model(input_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1bb827-1cdf-40b4-ad92-b40ede0d93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86c35aad-fbdf-41f4-ad57-ed7da9d71b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 119s 119s/step - loss: 1.5700 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 21s 21s/step\n",
      "1/1 [==============================] - 123s 123s/step - loss: 1.2114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d12fa206d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\n",
    "labels = [0, 3]\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.AlbertClassifier.from_preset(\n",
    "    \"albert_base_en_uncased\",\n",
    "    num_classes=4,\n",
    ")\n",
    "classifier.fit(x=features, y=labels, batch_size=2)\n",
    "classifier.predict(x=features, batch_size=2)\n",
    "\n",
    "# Re-compile (e.g., with a new learning rate).\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(5e-5),\n",
    "    jit_compile=True,\n",
    ")\n",
    "# Access backbone programmatically (e.g., to change `trainable`).\n",
    "classifier.backbone.trainable = False\n",
    "# Fit again.\n",
    "classifier.fit(x=features, y=labels, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13b34be0-9d07-42b1-be15-992bdb8106b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 176s 176s/step - loss: 1.5074 - sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d12b2d7bb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = {\n",
    "    \"token_ids\": np.ones(shape=(2, 12), dtype=\"int32\"),\n",
    "    \"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),\n",
    "    \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),\n",
    "}\n",
    "labels = [0, 3]\n",
    "\n",
    "# Pretrained classifier without preprocessing.\n",
    "classifier = keras_nlp.models.AlbertClassifier.from_preset(\n",
    "    \"albert_base_en_uncased\",\n",
    "    num_classes=4,\n",
    "    preprocessor=None,\n",
    ")\n",
    "classifier.fit(x=features, y=labels, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa2ace7a-fa31-46f7-99bb-7670f3458558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom backbone and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3067e1-0eb5-4900-95f8-f1b311535e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x000002D13F2195E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 54s 54s/step - loss: 1.3739 - sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d1284908e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\n",
    "labels = [0, 3]\n",
    "\n",
    "bytes_io = io.BytesIO()\n",
    "ds = tf.data.Dataset.from_tensor_slices(features)\n",
    "sentencepiece.SentencePieceTrainer.train(\n",
    "    sentence_iterator=ds.as_numpy_iterator(),\n",
    "    model_writer=bytes_io,\n",
    "    vocab_size=10,\n",
    "    model_type=\"WORD\",\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece=\"<pad>\",\n",
    "    unk_piece=\"<unk>\",\n",
    "    bos_piece=\"[CLS]\",\n",
    "    eos_piece=\"[SEP]\",\n",
    "    user_defined_symbols=\"[MASK]\",\n",
    ")\n",
    "tokenizer = keras_nlp.models.AlbertTokenizer(\n",
    "    proto=bytes_io.getvalue(),\n",
    ")\n",
    "preprocessor = keras_nlp.models.AlbertPreprocessor(\n",
    "    tokenizer=tokenizer,\n",
    "    sequence_length=128,\n",
    ")\n",
    "backbone = keras_nlp.models.AlbertBackbone(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    embedding_dim=128,\n",
    "    intermediate_dim=512,\n",
    "    max_sequence_length=128,\n",
    ")\n",
    "classifier = keras_nlp.models.AlbertClassifier(\n",
    "    backbone=backbone,\n",
    "    preprocessor=preprocessor,\n",
    "    num_classes=4,\n",
    ")\n",
    "classifier.fit(x=features, y=labels, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42d4dafe-fee9-4d5f-b8dc-602fe2241d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AlbertClassifier model from preset architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63a308e0-a01b-4916-8d65-d7aa6a458cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp.models import AlbertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b05d379-6a85-4d2f-a411-243b07a92a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load architecture and weights from preset\n",
    "model = AlbertClassifier.from_preset(\"albert_base_en_uncased\",num_classes=4)\n",
    "\n",
    "# Load randomly initialized model from preset architecture\n",
    "model = AlbertClassifier.from_preset(\"albert_base_en_uncased\",num_classes= 4,load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2facd-b282-4e74-80fc-94624c16615f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
