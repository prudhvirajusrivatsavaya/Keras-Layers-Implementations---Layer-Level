{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434f60a0-f646-4e6e-bd42-fb4ef0b3bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90341718-c232-4d81-9227-2ea9dedf6056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_base_en_uncased/v1/vocab.txt\n",
      "231508/231508 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int32, numpy=array([ 5,  6,  7,  8,  9, 10])>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unbatched input.\n",
    "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_base_en_uncased\",)\n",
    "tokenizer(\"The quick brown fox jumped.\")\n",
    "\n",
    "# Batched input.\n",
    "tokenizer([\"The quick brown fox jumped.\", \"The fox slept.\"])\n",
    "\n",
    "# Detokenization.\n",
    "tokenizer.detokenize(tokenizer(\"The quick brown fox jumped.\"))\n",
    "\n",
    "# Custom vocabulary.\n",
    "vocab = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "vocab += [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\"]\n",
    "tokenizer = keras_nlp.models.BertTokenizer(vocabulary=vocab)\n",
    "tokenizer(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e42cab-4bcb-45e4-a8f3-edf2f7e585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp.models import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c6ad3b-3952-496a-a818-024fdd6e0f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[unused4] [unused5] [unused6] [unused7] [unused8]'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a preset tokenizer.\n",
    "tokenizer = BertTokenizer.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "# Tokenize some input.\n",
    "tokenizer(\"The quick brown fox tripped.\")\n",
    "\n",
    "# Detokenize some input.\n",
    "tokenizer.detokenize([5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91dc902-c236-48d1-9948-0fd860b91b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d55e60-3f6b-4630-ad0b-39f75c3dfc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       " array([ 1,  5,  6,  7,  8,  9, 10,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3])>,\n",
       " 'segment_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])>,\n",
       " 'padding_mask': <tf.Tensor: shape=(512,), dtype=bool, numpy=\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "# Tokenize and pack a single sentence.\n",
    "preprocessor(\"The quick brown fox jumped.\")\n",
    "\n",
    "# Tokenize a batch of single sentences.\n",
    "preprocessor([\"The quick brown fox jumped.\", \"Call me Ishmael.\"])\n",
    "\n",
    "# Preprocess a batch of sentence pairs.\n",
    "# When handling multiple sequences, always convert to tensors first!\n",
    "first = tf.constant([\"The quick brown fox jumped.\", \"Call me Ishmael.\"])\n",
    "second = tf.constant([\"The fox tripped.\", \"Oh look, a whale.\"])\n",
    "preprocessor((first, second))\n",
    "\n",
    "# Custom vocabulary.\n",
    "vocab = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "vocab += [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\"]\n",
    "tokenizer = keras_nlp.models.BertTokenizer(vocabulary=vocab)\n",
    "preprocessor = keras_nlp.models.BertPreprocessor(tokenizer)\n",
    "preprocessor(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a5ccf4-c1b6-4cb4-b487-93c581f90596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
       " array([[1, 5, 6, ..., 3, 3, 3],\n",
       "        [1, 0, 0, ..., 3, 3, 3]])>,\n",
       " 'segment_ids': <tf.Tensor: shape=(2, 512), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])>,\n",
       " 'padding_mask': <tf.Tensor: shape=(2, 512), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False]])>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor((first, second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622aa29-6de3-4ce1-9fff-428b1087e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping with tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937a0647-6986-4fc6-a878-ee2f7b0fdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_base_en_uncased\"\n",
    ")\n",
    "\n",
    "first = tf.constant([\"The quick brown fox jumped.\", \"Call me Ishmael.\"])\n",
    "second = tf.constant([\"The fox tripped.\", \"Oh look, a whale.\"])\n",
    "label = tf.constant([1, 1])\n",
    "\n",
    "# Map labeled single sentences.\n",
    "ds = tf.data.Dataset.from_tensor_slices((first, label))\n",
    "ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map unlabeled single sentences.\n",
    "ds = tf.data.Dataset.from_tensor_slices(first)\n",
    "ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map labeled sentence pairs.\n",
    "ds = tf.data.Dataset.from_tensor_slices(((first, second), label))\n",
    "ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map unlabeled sentence pairs.\n",
    "ds = tf.data.Dataset.from_tensor_slices((first, second))\n",
    "# Watch out for tf.data's default unpacking of tuples here!\n",
    "# Best to invoke the `preprocessor` directly in this case.\n",
    "ds = ds.map(\n",
    "    lambda first, second: preprocessor(x=(first, second)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e5af8-0e6a-41f6-8085-ee853c16bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertBackbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "750972d1-0cb0-45fe-bec7-402cb962c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77256e46-5091-46ae-8bbc-15485b778f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_base_en_uncased/v1/model.h5\n",
      "438162680/438162680 [==============================] - 36s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence_output': <tf.Tensor: shape=(1, 12, 256), dtype=float32, numpy=\n",
       " array([[[-0.49211618, -0.02429868, -1.851578  , ...,  1.1772653 ,\n",
       "          -0.08377397, -1.9198692 ],\n",
       "         [-0.53546405, -0.06992134, -1.2401865 , ...,  0.38118008,\n",
       "           0.4098072 , -1.9182652 ],\n",
       "         [-0.8774007 , -1.276829  , -0.9952731 , ...,  0.12106248,\n",
       "           0.11782676, -2.1016893 ],\n",
       "         ...,\n",
       "         [-0.5384043 , -0.5705477 , -1.4774486 , ...,  0.22520651,\n",
       "          -0.05159385, -1.9838173 ],\n",
       "         [-1.1852553 , -0.7356736 , -1.9857584 , ...,  0.3143446 ,\n",
       "          -0.46081302, -1.6065874 ],\n",
       "         [-1.6154847 , -0.20775907, -1.6879659 , ...,  0.66859907,\n",
       "          -0.0133528 , -1.3697277 ]]], dtype=float32)>,\n",
       " 'pooled_output': <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       " array([[-0.30399308, -0.39208707, -0.31638876, -0.050208  , -0.4902698 ,\n",
       "          0.13524899,  0.40939018, -0.30869877, -0.19257137,  0.08193897,\n",
       "         -0.26379967,  0.11055775,  0.01061076,  0.08643863, -0.09605475,\n",
       "         -0.41675532, -0.24466585,  0.04769069,  0.24902795, -0.06853835,\n",
       "         -0.15495668, -0.31921238, -0.29117033, -0.2888775 ,  0.27349156,\n",
       "          0.25731322,  0.06708848,  0.3894322 ,  0.54235655,  0.05584031,\n",
       "         -0.14016359,  0.1001742 , -0.20370774,  0.37160006,  0.42774868,\n",
       "         -0.21795885,  0.07956587,  0.10209937, -0.2048376 , -0.16202018,\n",
       "          0.5282346 ,  0.03176535,  0.26786882,  0.07690137, -0.22521733,\n",
       "          0.36881194,  0.04695394, -0.27600494, -0.22383231, -0.06081233,\n",
       "          0.29574114, -0.26425585, -0.18032949, -0.01877533,  0.20268543,\n",
       "         -0.00072202,  0.10909183, -0.23334913,  0.07488288,  0.17219512,\n",
       "         -0.44639388, -0.23388202,  0.27164933, -0.30766603, -0.16825707,\n",
       "         -0.5426627 ,  0.02998337,  0.51800287, -0.22372654,  0.15765454,\n",
       "         -0.28805986,  0.3926458 , -0.04902286,  0.02073649,  0.31553346,\n",
       "          0.42231414,  0.02810436, -0.18085751,  0.11110559, -0.47425333,\n",
       "         -0.13088554, -0.0318822 , -0.10372857,  0.29457146, -0.14795299,\n",
       "         -0.04751422, -0.08763422,  0.01682022, -0.1505391 ,  0.04918926,\n",
       "          0.11446519,  0.36285138, -0.01121416, -0.29012296,  0.04054479,\n",
       "          0.23477016, -0.10526587,  0.14188926, -0.36657894,  0.26402292,\n",
       "         -0.11344411, -0.03775665,  0.2888421 , -0.59760165,  0.1899944 ,\n",
       "          0.16900086, -0.25574124, -0.24851702, -0.10481071, -0.5565951 ,\n",
       "          0.00520821,  0.44162285, -0.4079015 ,  0.01457493, -0.18682662,\n",
       "         -0.52251285, -0.25980383, -0.02892666, -0.16460317,  0.13432345,\n",
       "          0.2865954 ,  0.5276345 ,  0.06833741,  0.0575534 ,  0.2774926 ,\n",
       "         -0.09028371,  0.27071092,  0.52142644,  0.2170211 , -0.5747706 ,\n",
       "         -0.02170322, -0.07384833, -0.1403466 ,  0.42524496,  0.24388519,\n",
       "         -0.08911291,  0.16062263, -0.05246183,  0.12479532, -0.34646112,\n",
       "         -0.0815508 ,  0.4901017 , -0.01055112, -0.16453272,  0.4289983 ,\n",
       "          0.04686214, -0.07627854,  0.2637046 , -0.01858655,  0.09040594,\n",
       "         -0.02133297, -0.37070096, -0.14697398,  0.2845672 ,  0.26573503,\n",
       "          0.16570492, -0.41946802,  0.34591454, -0.2720391 ,  0.2079315 ,\n",
       "          0.00551462,  0.29351708, -0.1494426 , -0.2551514 ,  0.24068168,\n",
       "         -0.24822897,  0.54188544,  0.22621864, -0.17341653, -0.49769905,\n",
       "         -0.15922856,  0.06018794,  0.2030712 ,  0.1958358 ,  0.12625028,\n",
       "         -0.08128673,  0.49077633, -0.28146663,  0.10188606, -0.12851557,\n",
       "          0.16747798,  0.27229384, -0.2500988 ,  0.05359745,  0.18011795,\n",
       "          0.16027336, -0.24904865, -0.2235037 , -0.2371719 , -0.07019839,\n",
       "         -0.5689779 , -0.22793882,  0.14205708, -0.27168837,  0.67751867,\n",
       "         -0.38135755,  0.40709695,  0.56612915,  0.204557  , -0.41343984,\n",
       "         -0.5021297 ,  0.10918165, -0.1485946 ,  0.341723  ,  0.34960583,\n",
       "         -0.00646006,  0.32918638,  0.00922195, -0.06317433,  0.09976228,\n",
       "          0.10771981,  0.42223963, -0.3779093 ,  0.04110168,  0.27572653,\n",
       "          0.16711478,  0.45723665,  0.1646899 ,  0.31713182, -0.28810766,\n",
       "          0.15504599,  0.06066651, -0.10363713, -0.21852474,  0.63499165,\n",
       "          0.08804443,  0.27246767, -0.34539613, -0.01536599, -0.2837038 ,\n",
       "         -0.27541342, -0.5646982 ,  0.09315097,  0.36726162,  0.13565557,\n",
       "         -0.3055756 , -0.06972173, -0.04600345, -0.00641733, -0.14476752,\n",
       "          0.06986754, -0.04794768,  0.07034764,  0.45436454,  0.1182019 ,\n",
       "         -0.31837037,  0.11117389,  0.12070749,  0.359505  ,  0.07985863,\n",
       "         -0.5109178 ,  0.3194994 , -0.47927582,  0.13104916,  0.34816417,\n",
       "          0.13594955]], dtype=float32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "    \"token_ids\": np.ones(shape=(1, 12), dtype=\"int32\"),\n",
    "    \"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]]),\n",
    "    \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),\n",
    "}\n",
    "\n",
    "# Pretrained BERT encoder.\n",
    "model = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en_uncased\")\n",
    "model(input_data)\n",
    "\n",
    "# Randomly initialized BERT encoder with a custom config.\n",
    "model = keras_nlp.models.BertBackbone(\n",
    "    vocabulary_size=30552,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    intermediate_dim=512,\n",
    "    max_sequence_length=128,\n",
    ")\n",
    "model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401c0cd1-4eb4-431a-9b53-8cf2e72d607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased/v1/model.h5\n",
      "17602216/17602216 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load architecture and weights from preset\n",
    "model = keras_nlp.models.BertBackbone.from_preset(\n",
    "    \"bert_tiny_en_uncased\"\n",
    ")\n",
    "\n",
    "# Load randomly initialized model from preset architecture\n",
    "model = keras_nlp.models.BertBackbone.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    load_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6929f2-b8cf-4cc9-aa20-3453950a1841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
