{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a206bae-5163-4e36-8a0d-2d8faa05222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer, Dense, Activation, Masking, Embedding, LSTM, Flatten\n",
    "from keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334c0b26-5b9b-43d3-ae3d-ed10a24fedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "input_layer = InputLayer(input_shape=(20,))\n",
    "layer_1 = Embedding(input_dim=100, output_dim=512, input_length=20)\n",
    "layer_2 = LSTM(units=20, input_shape=(20,512))\n",
    "layer_3 = Dense(units = 256)\n",
    "layer_4 = Dense(units = 128)\n",
    "flatten = Flatten()\n",
    "layer_5 = Dense(3, activation='softmax')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(layer_1)\n",
    "model.add(layer_2)\n",
    "model.add(layer_3)\n",
    "model.add(layer_4)\n",
    "model.add(Flatten())\n",
    "model.add(layer_5)\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ed00ff-a2ae-4498-bab2-de618db335c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 512)           51200     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                42640     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               5376      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,499\n",
      "Trainable params: 132,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d686e54c-4318-4ad7-a862-54b3cd3ba95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.random.randint(0, 20, size=(1000, 20))\n",
    "output_data = np.random.randint(0, 3, size=(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6fe8f3-abf6-41ac-b443-f142d52b6acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 22ms/step - loss: 1.1040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2962549d490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_shape, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd94c09-cf11-4fed-bbe6-d4de59da508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ca23118-6ca4-4d8e-8865-af5a4ec5d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1088\\1841327636.py:72: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(layer.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0871\n",
      "Epoch [2/10], Loss: 1.0862\n",
      "Epoch [3/10], Loss: 1.1116\n",
      "Epoch [4/10], Loss: 1.0985\n",
      "Epoch [5/10], Loss: 1.1440\n",
      "Epoch [6/10], Loss: 1.0734\n",
      "Epoch [7/10], Loss: 1.1234\n",
      "Epoch [8/10], Loss: 0.9834\n",
      "Epoch [9/10], Loss: 1.0068\n",
      "Epoch [10/10], Loss: 0.8244\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate sample dataset\n",
    "input_data = np.random.randint(0, 20, size=(1000, 20))\n",
    "output_data = np.random.randint(0, 3, size=(1000,))\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x = torch.tensor(input_data, dtype=torch.float32)\n",
    "y = torch.tensor(output_data, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataset = TensorDataset(x, y)\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 20\n",
    "embedding_dim = 32\n",
    "hidden_size = 50\n",
    "output_size = 3  # 3 classes\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "weight_decay = 1e-5  # L2 regularization strength\n",
    "\n",
    "# Define LSTM model with regularization for specific layers\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "        # Apply L2 regularization to specific layers\n",
    "        self.init_weights(self.fc1)\n",
    "        self.init_weights(self.fc2)\n",
    "\n",
    "        # Apply separate initialization for each layer\n",
    "        self.init_weights(self.embedding)\n",
    "        self.init_weights(self.lstm)\n",
    "        self.init_weights(self.fc1)\n",
    "        self.init_weights(self.fc2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "    # To apply for all layers\n",
    "    def init_weights(self):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                layer.bias.data.fill_(0.01)\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "        elif isinstance(layer, nn.Embedding):\n",
    "            nn.init.xavier_normal(layer.weight)\n",
    "        elif isinstance(layer, nn.LSTM):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0.01)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LSTMModel(input_size, embedding_dim, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer with weight decay for regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs.long())\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_sequence = torch.randint(0, 20, (1, 20))  # Replace with your test sequence\n",
    "    output = model(test_sequence.long())\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47ab7930-ed46-417a-8863-d9ad9306a4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0341e-01, -1.5050e-01,  9.0722e-02,  1.6675e-02,  3.6886e-01,\n",
       "          -1.5278e-01,  1.0064e-01,  1.0081e-01, -1.0638e-01,  2.4625e-01,\n",
       "           1.9970e-01,  1.2338e-01,  1.5445e-01,  2.9942e-01, -3.2740e-01,\n",
       "           5.7533e-02,  2.8294e-01, -2.1169e-01,  7.9151e-02, -2.1043e-01,\n",
       "           4.1633e-02,  4.8168e-02, -5.2222e-02,  1.0055e-01, -9.9956e-02,\n",
       "          -4.2472e-02, -7.0215e-02,  3.2033e-02,  1.6674e-01,  5.4970e-02,\n",
       "          -9.3452e-02, -6.8117e-02, -3.2147e-02,  2.3390e-01, -1.8857e-01,\n",
       "          -3.9122e-02,  3.6725e-01, -7.9554e-02,  1.1202e-01, -9.5345e-02,\n",
       "           1.9811e-03, -8.9209e-02, -3.0315e-01,  1.4641e-01,  7.6128e-02,\n",
       "          -2.0324e-02,  1.3551e-01, -1.6896e-01,  2.5963e-01,  4.7893e-03],\n",
       "         [-5.1268e-01, -2.7056e-01,  1.8616e-01,  8.5120e-02,  3.9429e-01,\n",
       "           2.1055e-01, -7.5215e-02,  5.5014e-01, -1.5677e-01,  2.5060e-01,\n",
       "           3.2501e-01,  4.9107e-02,  9.2388e-02,  1.2554e-01, -9.4959e-02,\n",
       "          -2.0473e-01,  8.3703e-02, -2.3864e-01,  1.8820e-01,  3.6269e-02,\n",
       "           1.6144e-01,  1.6404e-01,  2.1904e-01,  3.5505e-01, -3.3291e-02,\n",
       "           1.3601e-02,  1.0454e-01,  1.2828e-01, -2.0678e-01,  1.1432e-01,\n",
       "           1.0456e-01,  3.5501e-02, -6.5577e-02,  1.4925e-02,  2.0614e-02,\n",
       "           2.6393e-03,  3.6407e-01,  2.0888e-02,  1.9093e-01,  1.8376e-01,\n",
       "           1.8282e-01,  3.3069e-01, -3.1453e-01,  2.0264e-01, -1.4388e-01,\n",
       "          -2.6609e-01,  2.0741e-01, -3.0826e-01,  6.0476e-01,  3.8087e-02],\n",
       "         [-5.1198e-01, -1.7487e-01,  1.3297e-01, -7.4371e-02,  2.7861e-01,\n",
       "           1.0328e-01, -4.0463e-01, -1.4051e-02,  8.0046e-02,  2.9287e-01,\n",
       "          -2.4391e-01, -3.6325e-02,  2.5058e-01, -3.6752e-02, -3.2266e-01,\n",
       "          -3.1865e-02,  7.4003e-02, -3.0426e-01,  6.6514e-01,  6.0486e-02,\n",
       "           1.7513e-01,  6.8671e-02,  6.6732e-02,  3.3768e-01, -4.0137e-01,\n",
       "           2.0310e-01,  4.0539e-02,  1.5769e-01,  1.9703e-01,  2.0632e-01,\n",
       "          -2.8243e-02, -2.5562e-01, -3.0638e-03,  3.8282e-01, -1.1285e-01,\n",
       "          -2.2424e-02,  3.9648e-01, -3.9469e-01,  4.6463e-02, -3.1152e-02,\n",
       "           1.7373e-01, -3.1613e-04, -1.0703e-01,  2.6585e-01,  4.6116e-02,\n",
       "          -2.4640e-01,  1.6658e-01, -1.4120e-01,  1.6938e-01,  1.3403e-02],\n",
       "         [-5.2622e-01, -1.5565e-02,  3.7985e-01, -2.7667e-02,  1.8661e-01,\n",
       "           1.8097e-01, -1.3468e-01, -2.1699e-01,  1.6113e-01,  4.1389e-01,\n",
       "           6.1880e-02,  1.2389e-01,  1.5695e-01,  1.7681e-01, -5.6556e-01,\n",
       "           7.2932e-02,  4.1956e-01, -2.5791e-01,  6.7315e-01,  2.1035e-01,\n",
       "          -7.1687e-03,  2.8663e-02, -1.1883e-01,  3.6444e-02,  1.1668e-01,\n",
       "           3.1564e-01,  3.8677e-01,  1.2014e-01,  5.4043e-01,  5.7124e-02,\n",
       "          -1.3543e-01,  1.8554e-02, -1.3213e-02,  6.3980e-03, -7.4463e-02,\n",
       "          -2.6374e-01,  4.9206e-01, -1.5885e-01, -6.7995e-02,  2.3948e-01,\n",
       "          -1.6780e-02,  9.6872e-02, -3.8755e-02,  3.2287e-01,  2.1039e-02,\n",
       "           9.5315e-02,  3.1228e-01,  8.1123e-02, -3.7304e-02,  4.8085e-02],\n",
       "         [-6.3649e-01, -2.9271e-01, -3.4573e-01,  1.9059e-02,  6.6206e-02,\n",
       "          -4.4114e-03, -1.6405e-01,  2.0918e-01, -9.5596e-02,  3.1865e-01,\n",
       "           9.5266e-02,  3.3459e-01,  2.1155e-01,  4.8208e-01, -1.2085e-01,\n",
       "           6.7129e-02,  3.1260e-01,  2.1772e-02,  6.7604e-01,  6.0745e-01,\n",
       "           7.8583e-02,  4.5354e-02, -2.9105e-01, -2.6251e-01,  2.1385e-01,\n",
       "           2.2853e-01,  4.1941e-01,  2.9833e-01,  3.5832e-01,  8.9826e-02,\n",
       "          -7.1179e-02,  2.0410e-01, -1.2959e-03, -1.9758e-01,  2.5377e-02,\n",
       "           1.1967e-01,  3.1091e-01, -5.0267e-01, -1.8128e-02,  5.5558e-01,\n",
       "           2.6041e-01,  9.9208e-02, -3.1873e-01,  4.2685e-01,  1.5131e-01,\n",
       "          -1.2991e-01,  2.9584e-01,  3.3489e-01,  5.7316e-02, -1.4333e-01],\n",
       "         [-5.7336e-01, -2.4040e-01,  4.7271e-02,  4.2204e-02,  5.1837e-01,\n",
       "          -4.5933e-01,  4.0879e-01,  1.7630e-01, -1.9264e-01,  3.2932e-01,\n",
       "           4.3271e-01,  3.2077e-01, -1.5578e-01,  1.8968e-01, -3.4374e-01,\n",
       "          -9.6057e-02, -7.7200e-02, -2.4564e-02,  2.9691e-01,  7.6954e-02,\n",
       "           2.7171e-02, -5.9477e-02, -5.2958e-01,  1.5796e-01, -1.0952e-01,\n",
       "          -9.0588e-02,  5.8548e-01,  3.1863e-01,  1.9251e-01,  1.2864e-01,\n",
       "          -1.7108e-01,  4.5276e-01, -3.9214e-03, -4.2057e-01,  2.7193e-01,\n",
       "          -1.0096e-01,  7.4957e-01, -6.0292e-01,  6.0999e-02,  5.0730e-01,\n",
       "           3.1273e-02,  5.8372e-01, -3.7846e-01,  1.7550e-01,  2.4091e-01,\n",
       "          -1.0958e-01,  1.9990e-01,  3.6896e-03,  3.1974e-01, -3.4140e-01],\n",
       "         [-6.5968e-01, -2.1586e-01,  3.1102e-01, -1.3866e-01,  3.8035e-01,\n",
       "          -1.3374e-01, -2.3444e-01,  1.4939e-01,  9.1802e-02,  4.5096e-01,\n",
       "          -8.3168e-02,  1.3046e-01,  2.8109e-01,  1.4552e-01, -5.2014e-01,\n",
       "           2.3888e-01,  1.6758e-02, -1.2213e-02,  7.1748e-01,  4.8330e-02,\n",
       "           1.4786e-01, -1.3160e-01, -2.0827e-01,  1.5955e-01, -5.8781e-01,\n",
       "          -2.1690e-03,  3.9383e-01,  4.9757e-01,  2.8796e-01,  2.1564e-01,\n",
       "          -1.5404e-01, -1.2360e-01,  3.9722e-02,  2.7934e-01, -8.3158e-02,\n",
       "          -1.2204e-01,  6.6770e-01, -6.3883e-01,  4.9797e-02,  4.1409e-01,\n",
       "           1.7323e-01,  1.9261e-01, -1.3864e-01,  3.6482e-01,  2.0553e-01,\n",
       "          -1.0584e-01,  1.4412e-01,  8.0088e-02,  4.6534e-02, -1.4557e-01],\n",
       "         [-7.1545e-01, -5.9975e-02,  2.7985e-01, -1.0811e-01, -1.4364e-01,\n",
       "          -1.4977e-01, -2.7791e-02,  4.1277e-01,  2.5627e-01,  5.0785e-01,\n",
       "           4.9703e-03,  1.6068e-01, -1.7416e-01,  2.6142e-01, -6.1375e-01,\n",
       "           1.8816e-02, -1.8079e-02,  1.7046e-01,  7.2831e-01,  2.6123e-01,\n",
       "           3.0786e-02, -1.5954e-01, -1.7483e-01, -1.9425e-01, -2.4761e-02,\n",
       "          -2.5023e-01,  7.4534e-02,  4.9490e-01,  3.9541e-01,  2.2468e-01,\n",
       "          -3.3072e-01,  1.8439e-01,  1.6017e-01, -4.7197e-02, -4.1093e-02,\n",
       "          -6.7708e-02,  5.6475e-01, -6.1127e-01, -1.1971e-01,  5.0800e-01,\n",
       "           4.0573e-01,  4.1777e-01, -8.4161e-02,  4.4319e-01,  2.7443e-01,\n",
       "           7.7773e-02,  5.2266e-02,  2.4134e-01, -3.2508e-01, -3.2040e-01],\n",
       "         [-6.0977e-01,  1.1669e-01,  3.1169e-01,  1.4628e-01, -5.4329e-01,\n",
       "          -3.8546e-02, -4.6220e-01,  1.5482e-01,  2.5788e-01,  5.9577e-01,\n",
       "          -3.4027e-01, -3.5750e-01, -4.0715e-01,  2.0565e-01, -4.5722e-01,\n",
       "           3.5177e-01, -3.5687e-01,  5.2379e-01,  5.6084e-01,  3.2373e-01,\n",
       "          -2.6272e-01, -5.3284e-01, -4.5447e-01, -4.5695e-01, -1.6864e-01,\n",
       "          -2.5991e-01,  8.8006e-02,  5.1557e-01, -1.5576e-01,  2.0204e-02,\n",
       "          -9.3440e-02, -1.2800e-02, -6.1949e-02, -2.8328e-01, -5.5471e-02,\n",
       "          -7.0634e-02,  2.7319e-01, -3.5948e-01, -4.5179e-01,  6.8402e-01,\n",
       "           7.6299e-01,  4.4272e-01,  2.4700e-01,  4.3558e-01,  2.3563e-01,\n",
       "           2.2735e-01, -7.2774e-02,  1.3252e-01, -5.0019e-01, -4.7709e-01],\n",
       "         [-2.0641e-01,  2.8588e-01,  5.4050e-01, -2.7863e-01, -6.4104e-01,\n",
       "          -2.7862e-01, -4.2379e-03,  2.4637e-02,  6.2797e-01,  2.6354e-01,\n",
       "          -2.6004e-01,  2.7282e-01, -7.6726e-02,  2.5060e-01, -1.5705e-01,\n",
       "          -8.3837e-02, -5.5549e-01,  7.5446e-01, -2.0406e-01,  5.9898e-02,\n",
       "          -2.2614e-01, -7.7398e-01, -3.3402e-01, -1.4688e-01, -3.2155e-01,\n",
       "          -3.5190e-01,  2.1707e-01,  7.6016e-01,  9.6033e-02, -2.0988e-01,\n",
       "          -2.7975e-02,  1.3440e-01, -2.0513e-01, -2.6689e-03,  2.0244e-01,\n",
       "          -4.3719e-01, -1.3901e-01, -3.5372e-01, -6.7560e-01,  5.3919e-01,\n",
       "          -5.5033e-02,  3.5213e-01,  4.3895e-01,  5.2312e-01,  2.2601e-01,\n",
       "           5.1954e-01, -4.9652e-01,  4.6079e-01, -6.3096e-01, -6.8728e-01],\n",
       "         [-2.9161e-02,  2.6912e-01,  1.3814e-01, -4.4868e-01, -2.1510e-01,\n",
       "          -6.2052e-02,  2.4326e-01, -5.7167e-01,  5.0038e-01,  2.4686e-01,\n",
       "          -1.1824e-01,  4.7125e-02,  5.2698e-01,  1.9755e-01, -7.9864e-02,\n",
       "          -3.7780e-01, -6.8719e-02,  4.0673e-01, -1.2947e-01,  1.3216e-01,\n",
       "          -3.9720e-01, -3.1145e-01, -1.4165e-01, -8.5986e-02, -1.6626e-01,\n",
       "           2.0678e-01, -2.0307e-01,  6.0097e-01, -2.7681e-01,  3.4675e-01,\n",
       "           5.2214e-01,  3.3827e-01, -2.5325e-01,  7.0823e-02, -7.1399e-02,\n",
       "           4.2293e-01, -4.1840e-02, -4.9225e-01, -1.7812e-01,  6.2659e-01,\n",
       "           3.2121e-01,  3.7610e-01, -2.9647e-02,  5.1000e-01,  6.3976e-01,\n",
       "           2.1630e-01,  9.1351e-02,  4.2961e-01, -6.5685e-01, -5.7385e-01],\n",
       "         [ 3.8457e-01,  5.4881e-01,  1.5101e-01, -4.8504e-03,  2.8080e-02,\n",
       "           1.5146e-01,  6.0707e-01,  1.4213e-02,  6.6618e-01, -1.5501e-02,\n",
       "          -2.4041e-01, -3.1665e-02,  5.6480e-01,  2.8302e-01, -3.0150e-01,\n",
       "          -1.5395e-01,  5.1368e-01,  1.0210e-01, -3.9605e-01, -8.5881e-02,\n",
       "          -5.5706e-01,  2.1466e-02,  3.8584e-02, -2.4766e-02, -6.7112e-02,\n",
       "           1.1249e-01,  1.1440e-01,  5.0797e-01, -9.8228e-02,  1.4555e-01,\n",
       "           7.5340e-01,  2.1241e-01, -1.1885e-01,  5.1947e-02, -3.0887e-01,\n",
       "           6.1590e-01,  2.4576e-02, -4.2075e-01,  2.7439e-02,  4.9672e-01,\n",
       "          -3.2380e-01,  4.3465e-01, -7.8633e-03,  2.3403e-01,  9.6065e-02,\n",
       "           6.7069e-01, -1.1162e-01, -5.1369e-02, -2.0136e-01,  2.1638e-03],\n",
       "         [-1.8894e-02,  4.9361e-01, -2.2795e-01,  5.5537e-01,  7.2586e-02,\n",
       "          -4.2289e-02,  4.1813e-01,  4.1328e-01,  5.8720e-01, -2.6585e-01,\n",
       "          -1.3024e-01, -1.2667e-01,  5.4045e-01,  3.4559e-01,  1.4724e-01,\n",
       "          -2.6134e-01,  4.0581e-01,  3.8798e-02, -2.4548e-01,  7.0495e-02,\n",
       "          -6.3669e-01,  3.3032e-01,  1.5926e-02,  7.9632e-02,  2.9378e-02,\n",
       "           1.1090e-01,  6.5092e-02,  9.1216e-02, -2.6682e-01, -7.1056e-02,\n",
       "           8.0599e-01,  2.9545e-01, -3.7110e-02, -7.4311e-02, -9.5937e-02,\n",
       "           7.0962e-01, -5.3861e-01, -1.3762e-01, -1.3385e-01,  1.8769e-01,\n",
       "          -7.2444e-01,  2.0926e-01,  1.7375e-01,  5.5408e-01,  3.4803e-01,\n",
       "           6.6361e-01, -3.6450e-01, -2.5518e-01, -7.0298e-01,  1.3278e-02],\n",
       "         [-1.3513e-02,  2.7141e-01, -2.4098e-01,  4.7423e-01, -7.0730e-02,\n",
       "          -1.1500e-01,  6.2459e-01,  5.0372e-01,  1.8509e-01, -4.4433e-01,\n",
       "          -5.4464e-01, -2.9488e-02,  6.7911e-02,  2.7531e-01, -3.5234e-01,\n",
       "          -1.8829e-01,  1.5061e-01,  1.5718e-01, -3.6670e-01,  4.6053e-01,\n",
       "          -2.1053e-01, -7.4917e-02, -3.3067e-01,  1.4042e-02, -9.9146e-02,\n",
       "          -1.0029e-01, -6.7734e-02,  5.7093e-02, -2.5214e-01,  2.9130e-02,\n",
       "           5.3371e-01,  2.8096e-01,  4.0880e-02, -5.4709e-02,  1.3103e-03,\n",
       "           5.8264e-01, -2.2676e-01, -1.0964e-01, -1.6424e-01,  1.4028e-01,\n",
       "          -7.5199e-01,  5.3259e-01, -4.0115e-02,  5.0816e-01,  2.3763e-01,\n",
       "           2.3767e-01,  3.1075e-02, -4.9799e-01, -2.7458e-01,  1.3649e-01],\n",
       "         [-9.5626e-02, -5.4035e-02,  5.3894e-02,  1.1815e-01,  2.9512e-01,\n",
       "          -1.9561e-01,  3.7052e-01,  2.1296e-01, -8.5296e-02, -4.7808e-01,\n",
       "          -1.4349e-01,  2.1211e-01,  4.0280e-01,  5.4901e-01, -6.0402e-01,\n",
       "          -1.4937e-01,  4.3737e-01, -3.1953e-01, -4.5275e-01,  1.4419e-01,\n",
       "          -8.7895e-02,  1.6894e-01,  3.5046e-01,  9.9039e-02, -2.0149e-01,\n",
       "          -1.7236e-01, -1.7171e-01,  2.4639e-02,  9.5404e-02, -8.0248e-03,\n",
       "          -4.8507e-02,  7.7400e-02,  1.9524e-02,  1.7596e-01, -3.0652e-01,\n",
       "           2.7368e-01,  3.9106e-01, -2.8920e-01,  9.5819e-02,  1.3473e-01,\n",
       "          -6.2527e-01,  2.1171e-01, -3.4757e-01,  5.2898e-01,  2.5257e-01,\n",
       "           2.6266e-01,  2.6737e-01, -5.8277e-01,  1.4465e-01,  2.5228e-01],\n",
       "         [-3.6464e-01,  2.5128e-02,  3.8592e-02, -1.4179e-01,  7.1654e-02,\n",
       "           3.1349e-02,  9.1987e-02,  3.5849e-01, -1.5263e-02, -2.4589e-01,\n",
       "          -1.2543e-01,  4.8875e-02,  2.0203e-01,  2.6494e-01, -5.9718e-01,\n",
       "           2.6471e-01,  2.3890e-01, -2.3541e-01,  3.9431e-02,  1.4907e-01,\n",
       "           1.2397e-01,  2.7523e-01,  2.6711e-01,  8.4904e-02, -1.8927e-01,\n",
       "          -3.9864e-02, -3.9085e-02,  9.5303e-02,  2.2038e-01,  4.6558e-03,\n",
       "          -8.4909e-02, -2.8197e-01,  9.0345e-02,  2.3800e-03, -2.4911e-01,\n",
       "           3.2432e-01,  3.5577e-01, -5.4675e-01,  5.3062e-02,  1.1684e-01,\n",
       "          -7.0329e-01,  1.0970e-01, -1.4546e-01,  4.3279e-01,  2.6718e-01,\n",
       "          -2.8617e-02, -5.9736e-02, -4.0419e-01, -2.3338e-02,  8.8598e-02],\n",
       "         [-3.9382e-01, -2.3656e-01, -1.8314e-01,  2.3307e-01,  9.5936e-02,\n",
       "          -1.6050e-01,  1.7456e-01,  6.9061e-01, -5.5998e-02, -2.8403e-01,\n",
       "          -6.2960e-02, -1.4337e-01,  2.3147e-01,  3.9490e-01,  1.7852e-01,\n",
       "          -1.1846e-01,  8.8842e-02, -1.6331e-01, -1.6545e-01,  3.0625e-01,\n",
       "          -5.6435e-02,  2.7959e-01, -5.7983e-02,  8.4256e-02,  1.2385e-01,\n",
       "           8.6492e-02,  4.9675e-02,  1.7517e-02,  1.5613e-01, -2.0776e-01,\n",
       "          -1.8266e-01,  2.4640e-02,  6.1184e-02, -1.0742e-01,  2.4543e-02,\n",
       "           5.9154e-01, -3.7024e-01, -1.2574e-01, -1.9331e-01,  1.6787e-01,\n",
       "          -7.2422e-01,  1.4370e-01,  1.8100e-01,  7.0552e-01,  2.9657e-01,\n",
       "           4.0796e-01, -3.0301e-01, -2.1276e-01, -4.2400e-01,  5.3681e-02],\n",
       "         [-4.7356e-01, -5.1103e-01, -5.0557e-01,  1.9717e-01,  3.2438e-01,\n",
       "          -2.2831e-01,  3.3115e-02,  7.0952e-01, -9.2652e-02, -7.2894e-01,\n",
       "           1.4521e-01,  2.0015e-01,  2.0656e-01,  5.7464e-01,  4.8564e-01,\n",
       "          -4.7029e-02,  2.6598e-01,  5.2318e-02,  3.2935e-01,  5.8186e-01,\n",
       "           1.0049e-01,  1.5989e-01, -3.3412e-02, -2.5407e-01,  2.8602e-01,\n",
       "          -3.6728e-02,  4.0630e-01,  1.9792e-01,  1.2721e-01, -3.9729e-01,\n",
       "          -1.2025e-01,  1.4289e-01,  6.6276e-02, -3.2864e-01,  1.5735e-01,\n",
       "           2.1962e-01, -9.3690e-02, -4.7633e-01, -2.3141e-01,  5.8330e-01,\n",
       "          -2.9504e-01,  2.7176e-01,  2.7286e-02,  5.2718e-01,  2.7645e-01,\n",
       "          -7.3984e-03, -7.6731e-02, -3.6003e-01,  2.3797e-02, -8.2948e-02],\n",
       "         [ 2.0663e-02, -3.9281e-01, -6.6064e-01,  3.5157e-01, -8.1370e-02,\n",
       "          -4.4089e-01,  2.2944e-01,  6.6297e-01, -1.9444e-03, -4.2040e-01,\n",
       "           2.4910e-01,  3.1562e-01, -3.4619e-01,  2.9228e-01,  4.6890e-01,\n",
       "          -1.9598e-02, -4.2060e-02,  3.7664e-01,  6.0172e-01,  3.9037e-01,\n",
       "           1.2576e-01,  1.1694e-01, -3.0266e-01, -3.6808e-01,  2.3578e-01,\n",
       "          -2.1907e-01,  4.2973e-02,  3.5332e-01,  1.8395e-01, -4.4324e-01,\n",
       "          -3.8424e-01,  1.7495e-01,  2.1165e-01, -3.7512e-01, -1.3308e-02,\n",
       "          -1.8658e-02,  1.6583e-01, -4.2105e-01, -2.6976e-01,  6.0823e-01,\n",
       "           3.1537e-01,  5.6941e-01, -1.5630e-01,  2.4872e-01,  6.0574e-02,\n",
       "           9.3093e-02,  8.6467e-03,  1.5535e-01,  1.3037e-02, -4.5565e-01],\n",
       "         [-2.7094e-01, -1.3741e-01, -1.6087e-01,  2.9200e-01,  5.1929e-01,\n",
       "          -6.9042e-01,  4.7479e-01,  2.5674e-01, -2.2337e-01, -6.1805e-01,\n",
       "           5.5467e-01,  4.7320e-01, -4.9256e-01,  1.0552e-01,  1.3275e-01,\n",
       "          -2.2390e-01, -4.7331e-01,  2.1789e-01,  7.4844e-02, -1.6348e-01,\n",
       "          -3.7451e-02, -6.1028e-02, -7.0812e-01, -1.1586e-01, -1.8449e-01,\n",
       "          -1.6066e-01,  5.3980e-01,  3.4296e-01,  1.1476e-01, -1.1466e-01,\n",
       "          -2.0624e-01,  4.2468e-01,  2.8886e-01, -6.8646e-01,  3.0540e-01,\n",
       "          -1.6490e-01,  6.9437e-01, -5.3200e-01,  4.7016e-02,  4.5873e-01,\n",
       "           7.1146e-02,  6.8491e-01, -5.5039e-01,  6.5147e-02,  9.5514e-02,\n",
       "           3.7420e-01, -1.1963e-01, -2.5807e-01,  3.4567e-01, -5.9318e-01]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09cbbe81-6054-4d3c-9ed1-9d22b449e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2709, -0.1374, -0.1609,  0.2920,  0.5193, -0.6904,  0.4748,  0.2567,\n",
       "         -0.2234, -0.6180,  0.5547,  0.4732, -0.4926,  0.1055,  0.1328, -0.2239,\n",
       "         -0.4733,  0.2179,  0.0748, -0.1635, -0.0375, -0.0610, -0.7081, -0.1159,\n",
       "         -0.1845, -0.1607,  0.5398,  0.3430,  0.1148, -0.1147, -0.2062,  0.4247,\n",
       "          0.2889, -0.6865,  0.3054, -0.1649,  0.6944, -0.5320,  0.0470,  0.4587,\n",
       "          0.0711,  0.6849, -0.5504,  0.0651,  0.0955,  0.3742, -0.1196, -0.2581,\n",
       "          0.3457, -0.5932]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66f5f1df-e31f-45f4-a4c7-7edb6c5fe84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(32, 50, batch_first=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
